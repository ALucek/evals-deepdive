{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8898b7e2-c7cf-49e4-8ee3-81d071736a4b",
   "metadata": {},
   "source": [
    "# LLM Application Evaluations\n",
    "\n",
    "**Problem:** LLM Applications are very new, and have limited resources for evaluating performance. LLM's are dynamic in their output, and thus require very custom evaluations, many overlook this step or do their actual testing in production with user feedback. This [doesn't always work out...](https://twitter.com/ChrisJBakke/status/1736533308849443121)\n",
    "\n",
    "**Solution:** We will be going over ways to utilize LangChain's offering [LangSmith](https://docs.smith.langchain.com/), a seperate software that allows tracing, testing, and evaluation of LLM applications.\n",
    "\n",
    "### LangChain's phenomenal summary of the LLM evaluation landscape:\n",
    "\n",
    "![x](evals_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43f4e9-38ae-4b05-afc1-d9d973d71776",
   "metadata": {},
   "source": [
    "---\n",
    "### Topics Covered\n",
    "\n",
    "#### Adding a Dataset to Langsmith\n",
    "- Adding the HuggingFace GO_Emotions Dataset to LangSmith\n",
    "#### Creating and Running Custom Evaluator to Compare LLM Outputs on Classification Task\n",
    "- Creating a custom evaluator to determine quality of output from different LLM's on emotion classification task for comparison\n",
    "#### Using an LLM as an Evaluator\n",
    "- Using an LLM-as-judge flow for Evaluating an LLM's ability for Question & Answer flows from a dataset made from a blog post\n",
    "- Both evaluating and comparing OpenAI and Mistral 7b's ability to answer questions\n",
    "#### Overview of Built-In w/LangChainStringEvaluator\n",
    "- Chain of Thought QA for contextual accuracy for both GPT-4o and Mistral 7b, then comparing the two\n",
    "- Using Built-In Criteria, Helpfulness\n",
    "#### LLM as an Evaluator with Custom Criteria\n",
    "- Unlabeled objectivity, having an LLM evaluate output without a grounded reference\n",
    "- Labeled objectivity, having an LLM evaluate output WITH a grounded reference\n",
    "  - Both as range scores and binary scores\n",
    "#### Evaluating Existing Evaluations (Summary Evaluation)\n",
    "- Running evaluations on entire experiments, not just each example\n",
    "- Pass test/fail test for overall evaluation score example with Mistral 7b\n",
    "#### Pairwise Evaluations (Comparing Experiments Against Each Other)\n",
    "- Evaluation for comparing two experiments outputs\n",
    "- LLM as showing preference towards one output vs another and comparing\n",
    "#### Unit Tests\n",
    "- Attaching decorators to pytest tests for evaluation in LangSmith\n",
    "- Both assertations, and for custom tests like embedding distance, edit distance, contains etc that work better for LLM output\n",
    "#### Evaluating Specific Parts of Existing Workflows\n",
    "- How to plug into specific parts of an overall LLM application workflow and run custom evaluations on the different steps\n",
    "- Adding on a few evaluations to my llama3 web research agent to evaluate document retrieval relevancy, and hallucination measurement\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0549a3d-0df7-4e7c-b4c7-66e36bfe5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Eval Testing 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f19020-3ca3-4fe6-bcd7-39d8ef50bd36",
   "metadata": {},
   "source": [
    "# DataSets\n",
    "\n",
    "### Importing a HuggingFace Dataset into LangSmith\n",
    "\n",
    "Going to be using one that I have previously used for fine tuning: \n",
    "\n",
    "https://huggingface.co/datasets/go_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdb351e-bba5-45f9-85a7-d348e7347a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omg i hope this is about [NAME]. I would LOVE ...</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finale</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which suggests nothing in itself. The same mod...</td>\n",
       "      <td>anger, annoyance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I double dog dare him.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Believe you me. TLJ is much, much worse.</td>\n",
       "      <td>disappointment, disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment            emotion label\n",
       "0  Omg i hope this is about [NAME]. I would LOVE ...                 optimism\n",
       "1                                             Finale                  neutral\n",
       "2  Which suggests nothing in itself. The same mod...         anger, annoyance\n",
       "3                             I double dog dare him.                  neutral\n",
       "4           Believe you me. TLJ is much, much worse.  disappointment, disgust"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Import\n",
    "\n",
    "# Importing with huggingface datasets package\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset('go_emotions')\n",
    "\n",
    "# creating an emotion index label dictionary\n",
    "label_index = {\n",
    "    \"0\": \"admiration\",\n",
    "    \"1\": \"amusement\",\n",
    "    \"2\": \"anger\",\n",
    "    \"3\": \"annoyance\",\n",
    "    \"4\": \"approval\",\n",
    "    \"5\": \"caring\",\n",
    "    \"6\": \"confusion\",\n",
    "    \"7\": \"curiosity\",\n",
    "    \"8\": \"desire\",\n",
    "    \"9\": \"disappointment\",\n",
    "    \"10\": \"disapproval\",\n",
    "    \"11\": \"disgust\",\n",
    "    \"12\": \"embarassment\",\n",
    "    \"13\": \"excitement\",\n",
    "    \"14\": \"fear\",\n",
    "    \"15\": \"gratitude\",\n",
    "    \"16\": \"grief\",\n",
    "    \"17\": \"joy\",\n",
    "    \"18\": \"love\",\n",
    "    \"19\": \"nervousness\",\n",
    "    \"20\": \"optimism\",\n",
    "    \"21\": \"pride\",\n",
    "    \"22\": \"realization\",\n",
    "    \"23\": \"relief\",\n",
    "    \"24\": \"remorse\",\n",
    "    \"25\": \"sadness\",\n",
    "    \"26\": \"surprise\",\n",
    "    \"27\": \"neutral\"\n",
    "}\n",
    "\n",
    "# Pull some random 20 Comments & Emotion\n",
    "data = []\n",
    "for i in range(1001, 1022):\n",
    "    comment = df['train'][i]['text']\n",
    "    label_indices = df['train'][i]['labels']\n",
    "\n",
    "    # deal with labels\n",
    "    if not isinstance(label_indices, list):\n",
    "        label_indices = [label_indices]\n",
    "\n",
    "    # label mapping\n",
    "    emotions = ', '.join([label_index.get(str(label)) for label in label_indices])\n",
    "\n",
    "    data.append((comment, emotions))\n",
    "\n",
    "comments_df = pd.DataFrame(data, columns=[\"comment\", \"emotion label\"])\n",
    "\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25b6398-d003-413f-9609-d9c283378821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting dataset into langsmith\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"go_emotions\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Social Media Comment and Emotion from HuggingFace Go Emotions\"\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"comment\": q} for q in comments_df['comment']],\n",
    "    outputs=[{\"emotion\": a} for a in comments_df['emotion label']],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777445b6-e2ef-489d-b5d3-c3454442e9f7",
   "metadata": {},
   "source": [
    "---\n",
    "# 1: Comparing Models with Custom Evaluation on LangChain Chain\n",
    "\n",
    "Classification Task of emotions with GPT-4o, GPT-3.5-T & Fine Tuned GPT-3.5-T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106bc1d-52f2-4843-af5e-732320b0158a",
   "metadata": {},
   "source": [
    "### Setting up First \"LLM App\"\n",
    "\n",
    "Emotion classification chain. Take in a social media comment, apply one of 27 emotion labels or neutral to it. \n",
    "\n",
    "We will be setting up this chain with 3 models. Base GPT-4o, Base GPT-3.5-Turbo, Fine Tuned GPT-3.5-Turbo on the Go Emotions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32c42a6-d0cd-4d41-81b9-657cc88133df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up Chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "emotion_analysis_template = \"\"\"\n",
    "You are a cutting edge emotion analysis classification assistant.\\\n",
    "You analyze a comment, and apply one or more emotion labels to it. \\\n",
    "\n",
    "The emotion labels are detailed here: \\\n",
    "\n",
    "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "\n",
    "Your output should simply be just the respective emotion, and if there are multiple seperated with commas. \\\n",
    "\n",
    "The comment is here: {comment}\n",
    "\"\"\"\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# different models to plug in (plus the fine tuned one!)\n",
    "gpt4o_llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o\")\n",
    "ft_llm = ChatOpenAI(temperature=0.0, model=\"ft:gpt-3.5-turbo-0125:personal:go-emotions:95jDha5f\")\n",
    "gpt35t_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "emotion_analysis_prompt = ChatPromptTemplate.from_template(emotion_analysis_template)\n",
    "\n",
    "analysis_chain_gpt35t = (\n",
    "    {\"comment\": RunnablePassthrough()} \n",
    "    | emotion_analysis_prompt\n",
    "    | gpt35t_llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "analysis_chain_gpt4o = (\n",
    "    {\"comment\": RunnablePassthrough()} \n",
    "    | emotion_analysis_prompt\n",
    "    | gpt4o_llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "analysis_chain_ft = (\n",
    "    {\"comment\": RunnablePassthrough()} \n",
    "    | emotion_analysis_prompt\n",
    "    | ft_llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4e1d5-b7e5-46fe-880c-f195f394e9ef",
   "metadata": {},
   "source": [
    "### Defining a custom evaluator\n",
    "\n",
    "Currently we have two pieces of data\n",
    "1. The dataset social media comment\n",
    "2. The dataset assigned emotion label(s)\n",
    "\n",
    "Want to evaluate model performance on the (1)Dataset social media comment in comparison to the (2)dataset assigned emotion label.\n",
    "\n",
    "The below function assigns an \"is_same\" score of 1 if it's an exact match, 0.5 if the LLM output partially contains the expected label, or 0 if nothing is included, this is returned as a dictionary with a key and score.\n",
    "\n",
    "To set this up, we have to specify the `Run` and `Example`. `Run` is the LLM \"run\" being evaluated, whereas `Example` is what's in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2779b33d-e071-4020-9865-d717351bd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "def expected_eval(run: Run, example: Example) -> dict:\n",
    "    # Getting the emotions and response as a set \n",
    "    expected_answer = set(example.outputs.get(\"emotion\").split(\", \"))\n",
    "    response = set(run.outputs.get(\"output\").split(\", \"))\n",
    "\n",
    "    # Check if response matches the expected answer exactly\n",
    "    if response == expected_answer:\n",
    "        return {\"key\": \"is_same\", \"score\": 1}\n",
    "    # Check if there is any overlap (partial match)\n",
    "    elif response & expected_answer:\n",
    "        return {\"key\": \"is_same\", \"score\": 0.5}\n",
    "    # No overlap at all\n",
    "    else:\n",
    "        return {\"key\": \"is_same\", \"score\": 0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ece44-385c-4f5d-bd90-f37f7d60c74d",
   "metadata": {},
   "source": [
    "### Using evaluate() to run your evaluations\n",
    "\n",
    "evaluate() needs a few arguments, the function (or in this case the chain) to evaluate, the dataset to compare against, the evaluator(s) as a list (can run multiple at a time, hence list), an experiment prefix for identification, and can pass in any metadata as a dictionary.\n",
    "\n",
    "#### Evaluating is_same score on base gpt-3.5-turbo output against go_emotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f615d21d-107f-4934-8943-9ff082b6d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-gpt35t-expected_answer-c71af42f' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/ab3b5df2-9b7c-42ae-85b2-5a1a3e6bd96d/compare?selectedSessions=b00fe5eb-67c1-4d86-8530-d3378fc7215b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34a5af06e7340b0b9cc92d7b606128e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluators\n",
    "qa_evaluator = [expected_eval]\n",
    "dataset_name = 'go_emotions'\n",
    "\n",
    "# Base Model gpt-3.5-turbo Run\n",
    "base_gpt35t_eval = evaluate(\n",
    "    analysis_chain_gpt35t.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-gpt35t-expected_answer\",\n",
    "    metadata={\n",
    "        \"variant\": \"base model gpt-3.5-turbo\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ed570-0b32-4b42-81e6-7508c5ef6007",
   "metadata": {},
   "source": [
    "#### Evaluating is_same score on base gpt-4o output against go_emotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a88f287-37b3-4b3a-9168-7c61f711daf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-gpt4o-expected_answer-2cd9e55d' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/ab3b5df2-9b7c-42ae-85b2-5a1a3e6bd96d/compare?selectedSessions=e966e0f8-66cb-470c-8f7b-8e16b104b6da\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717b86c950964ad2b3c7f5b14f169e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Base Model gpt-4o Run\n",
    "base_gpt4o_eval = evaluate(\n",
    "    analysis_chain_gpt4o.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-gpt4o-expected_answer\",\n",
    "    metadata={\n",
    "        \"variant\": \"base model gpt-4o\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1008f-157a-4c01-8ae0-a3dbc77bc4bc",
   "metadata": {},
   "source": [
    "#### Evaluating is_same score on fine tuned gpt-3.5-turbo output against go_emotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b78bf78-f982-4615-af2e-8b1e0d092631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-ft-3.5t-expected_answer-12892071' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/ab3b5df2-9b7c-42ae-85b2-5a1a3e6bd96d/compare?selectedSessions=6504ca10-7e42-4c56-a4c8-c29264025a5a\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17506acabedf4a68b1de9eb8534732ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Base Model fine-tuned gpt-3.5-turbo Run\n",
    "ft_gpt35t_eval = evaluate(\n",
    "    analysis_chain_ft.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-ft-3.5t-expected_answer\",\n",
    "    metadata={\n",
    "        \"variant\": \"fine tuned gpt-3.5-turbo\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdf47f-2370-42c5-90f4-df194450c6d3",
   "metadata": {},
   "source": [
    "---\n",
    "# 2: Assessing Model Output Using an LLM-As-Judge Approach\n",
    "\n",
    "Using built in evaluators to assess model performance, using another model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2daff1-108a-4d8a-8683-3b9e3c7e7b3d",
   "metadata": {},
   "source": [
    "#### Creating a new dataset of question and answer pairs\n",
    "\n",
    "Website of interest: https://lilianweng.github.io/posts/2023-06-23-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7517c95e-07ec-46c6-9885-5a58c786c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading A Web Page\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "text = [p.text for p in soup.find_all('p')]\n",
    "full_text = '\\n'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd4fc57-ff83-4de7-8ac4-814c4bf1d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Questions\n",
    "inputs = [\n",
    "    \"What is the primary function of LLM in autonomous agents?\",\n",
    "    \"Can you describe the role of 'Planning' in LLM-powered autonomous agents?\",\n",
    "    \"What types of memory are utilized by LLM-powered agents?\",\n",
    "    \"How do autonomous agents use tool APIs?\",\n",
    "    \"What are some challenges faced by LLM-powered autonomous agents in real-world applications?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"LLM functions as the core controller or 'brain' of autonomous agents, enabling them to handle complex tasks through planning, memory, and tool use.\",\n",
    "    \"In LLM-powered agents, 'Planning' involves breaking down complex tasks into manageable subgoals, reflecting on past actions, and refining strategies for improved outcomes.\",\n",
    "    \"LLM-powered agents utilize short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods, often leveraging external vector stores.\",\n",
    "    \"Autonomous agents use tool APIs to extend their capabilities beyond the model's weights, allowing access to current information, code execution, and proprietary data.\",\n",
    "    \"Challenges include managing the complexity of task dependencies, maintaining the stability of model outputs, and ensuring efficient interaction with external models and APIs.\"\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0feea2e5-c9b3-450a-b1ce-c275d173d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting dataset into langsmith\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"agent_dataset\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs Lilian Weng's AI Agents Blog Post.\"\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c18d9-4fcf-418b-b04f-1252006de752",
   "metadata": {},
   "source": [
    "### Defining \"apps\" to test\n",
    "\n",
    "Two LLM \"apps\" to be tested. Both are simple Question and Answering setups, with the context of the web page above inserted into the prompt.\n",
    "1. OpenAI gpt-4o Q/A\n",
    "2. Mistral 7b Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a734444-6d0b-4a62-9191-a0ff7f67bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "def qa_oai(inputs: dict) -> dict:\n",
    "    system_msg = f\"Answer the user's question in 2-3 sentences using this context: \\n\\n\\n {full_text}\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": inputs[\"question\"]}]\n",
    "\n",
    "    response = openai_client.chat.completions.create(messages=messages, model=\"gpt-4o\")\n",
    "\n",
    "    return {\"answer\": response.dict()['choices'][0]['message']['content']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1554ec4-90b9-489f-97bc-edb31473f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API\n",
    "import ollama\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_ollama(messages, model: str):\n",
    "    stream = ollama.chat(messages=messages, model='mistral', stream=True)\n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        response = response + chunk['message']['content']\n",
    "    return response\n",
    "\n",
    "def qa_mistral(inputs: dict) -> dict:\n",
    "    system_msg = f\"Answer the user's question using this context: \\n\\n\\n {full_text}\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": f'Answer the question in 2-3 sentences {inputs[\"question\"]}' }]\n",
    "    \n",
    "    response = call_ollama(messages, model=\"mistral\")\n",
    "\n",
    "    return {\"answer\": response} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7800a-4ff9-418d-8bdb-e456d1e1cdf4",
   "metadata": {},
   "source": [
    "### We can now use a built in functionality, the `LangChainStringEvaluator`\n",
    "\n",
    "https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations\n",
    "\n",
    "LangChainStringEvaluator has many built in evaluators. And essentially... evaluates a string based on different `criteria`. \n",
    "\n",
    "| Evaluator Name          | Output Key               | Simple Code Example                                                                                      |\n",
    "|-------------------------|--------------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| QA                      | correctness              | `LangChainStringEvaluator(\"qa\")`                                                                        |\n",
    "| Contextual Q&A          | contextual accuracy      | `LangChainStringEvaluator(\"context_qa\")`                                                                |\n",
    "| Chain of Thought Q&A    | cot contextual accuracy  | `LangChainStringEvaluator(\"cot_qa\")`                                                                    |\n",
    "| Criteria                | Depends on criteria key  | `LangChainStringEvaluator(\"criteria\", config={ \"criteria\": <criterion> })`                              |\n",
    "| Labeled Criteria        | Depends on criteria key  | `LangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": <criterion> })`                      |\n",
    "| Score                   | Depends on criteria key  | `LangChainStringEvaluator(\"score_string\", config={ \"criteria\": <criterion>, \"normalize_by\": 10 })`      |\n",
    "| Labeled Score           | Depends on criteria key  | `LangChainStringEvaluator(\"labeled_score_string\", config={ \"criteria\": <criterion>, \"normalize_by\": 10 })` |\n",
    "| Embedding Distance      | embedding_cosine_distance| `LangChainStringEvaluator(\"embedding_distance\")`                                                        |\n",
    "| String Distance         | string_distance          | `LangChainStringEvaluator(\"string_distance\", config={\"distance\": \"damerau_levenshtein\" })`              |\n",
    "| Exact Match             | exact_match              | `LangChainStringEvaluator(\"exact_match\")`                                                               |\n",
    "| Regex Match             | regex_match              | `LangChainStringEvaluator(\"regex_match\")`                                                               |\n",
    "| Json Validity           | json_validity            | `LangChainStringEvaluator(\"json_validity\")`                                                             |\n",
    "| Json Equality           | json_equality            | `LangChainStringEvaluator(\"json_equality\")`                                                             |\n",
    "| Json Edit Distance      | json_edit_distance       | `LangChainStringEvaluator(\"json_edit_distance\")`                                                        |\n",
    "| Json Schema             | json_schema              | `LangChainStringEvaluator(\"json_schema\")`                                                               |\n",
    "\n",
    "\n",
    "`criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.\n",
    "\n",
    "Or, you may define your own criteria in a custom dict as follows:\n",
    "`{ \"criterion_key\": \"criterion description\" }`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e45740-c8ba-4040-a5f3-c2ad7a3e272e",
   "metadata": {},
   "source": [
    "For our evaluation, going to use `[LangChainStringEvaluator(\"cot_qa\")]` for Chain of Thought contextual accuracy on question and answering. This will compare the LLM generated response to the question with the expected answer from the dataset, using a built in CoT chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc07ae1-509c-4317-a3c5-4a1f18f1c802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-qa-oai-641102aa' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=4af55dee-9be5-4973-9a56-f16c87ae65aa\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de31ee911d34df29aaa87194c020e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"agent_dataset\"\n",
    "\n",
    "oai_cot_eval = evaluate(\n",
    "    qa_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-agent-qa-oai\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with gpt-4o\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70251d04-1061-4a6f-a939-c8831f0f5713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-qa-mistral-6881f27b' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=07c0e0f4-fee9-42da-ac77-b6cf914997c8\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93601f9d96794fc1b19eece0da5c8df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Planning plays a crucial role in LLM-powered autonomous agents as it enables them to adjust their actions based on long-term goals and unexpected errors. However, current LLMs face challenges in planning over extended periods and decomposing tasks effectively, making them less robust compared to humans. Techniques such as self-reflection, vector search, tool augmentation, and reinforcement learning are being explored to enhance the planning capabilities of LLMs. LLM-powered agents utilize different types of memory, including dynamic memory for storing and reflecting on past experiences, and external knowledge sources such as databases or APIs for accessing additional information. Some agents also incorporate vector stores and retrieval systems for efficient access to large knowledge pools. Autonomous agents use tool APIs by integrating them with large language models, allowing the agents to access external knowledge and perform specific tasks more efficiently. This enables the agents to expand their capabilities beyond their internal knowledge base and improve their problem-solving abilities. Examples of tool APIs include database access, web search engines, and scientific research tools. The primary function of a Large Language Model (LLM) in autonomous agents is to process natural language instructions and generate responses or actions based on that input. It serves as the brain of the agent, interpreting data from external components and making decisions through reasoning and problem-solving capabilities. Some challenges faced by LLM-powered autonomous agents in real-world applications include finite context length, which limits historical information and instruction inclusion; difficulties in long-term planning and task decomposition; and the reliability of natural language interfaces due to potential formatting errors or rebellious behavior from the models."
     ]
    }
   ],
   "source": [
    "mistral_cot_eval = evaluate(\n",
    "    qa_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-agent-qa-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with Mistral 7b\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec334e-0e66-4e2e-b281-276cb295086c",
   "metadata": {},
   "source": [
    "### Trying one more out built in criteria, helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b0f316-a21b-4da1-90a0-838f4bd181a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-qa-oai-helpfulness-3361d6cb' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=1a6be94f-4f7b-4ef9-863c-ed2be1d3b441\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf3dc4b99d84108b9b92e3f420c8daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oai_helpfulness_eval = evaluate(\n",
    "    qa_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=[LangChainStringEvaluator(\"criteria\", config={ \"criteria\": \"helpfulness\" })],\n",
    "    experiment_prefix=\"test-agent-qa-oai-helpfulness\",\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with gpt-4o, Helpfulness check\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451c9a31-dce9-4180-a8eb-a334d3680155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-qa-mistral-helpfulness-d223aa3b' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=7245e36d-b143-4cfa-a15c-9f5714885636\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec5ff558e68484995752351ac5ddec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Autonomous agents utilize tool APIs by integrating them with large language models, enabling the agents to execute specific tasks and access external knowledge sources. This modular architecture enhances the capabilities of the agents, allowing them to perform complex operations and interact with various systems. (References: [11], [15], [17], [20]) In LLM-powered autonomous agents, planning plays a crucial role in enabling long-term goal achievement and effective task decomposition. It allows the agent to adjust plans when faced with unexpected errors, making it more robust compared to humans who learn from trial and error. However, current LLMs struggle with reliably generating accurate and formatted outputs for interface communication, which can limit their planning capabilities. Ongoing research focuses on improving these aspects, such as incorporating feedback mechanisms, synergizing reasoning and acting, and developing modular architectures that combine large language models with external knowledge sources and discrete reasoning. LLM-powered agents typically utilize two types of memory: internal memory, which is the agent's own knowledge base and working memory for storing information relevant to the current task, and external memory or databases, which store additional information that the agent may need to access during problem solving. The choice and combination of these memory types depend on the specific use case and design of the LLM-powered agent. In autonomous agents, Large Language Models (LLMs) serve as the intelligent core that processes natural language instructions and generates responses or actions based on the given context. They enable the agent to understand, reason, and generate human-like text, making them essential for tasks involving communication, problem solving, and interaction with external components. Some challenges faced by LLM-powered autonomous agents in real-world applications include finite context length, which limits the inclusion of historical information and detailed instructions, and reliability of natural language interfaces, as LLMs may make formatting errors or exhibit rebellious behavior. These limitations impact planning capabilities, task decomposition, and overall robustness of the agents. (Refer to [1-20] for more details.)"
     ]
    }
   ],
   "source": [
    "mistral_helpfulness_eval = evaluate(\n",
    "    qa_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=[LangChainStringEvaluator(\"criteria\", config={ \"criteria\": \"helpfulness\" })],\n",
    "    experiment_prefix=\"test-agent-qa-mistral-helpfulness\",\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with Mistral 7b, helpfulness check\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd9178-d45b-4455-abb0-f590c2ab5ee7",
   "metadata": {},
   "source": [
    "---\n",
    "# Digging in Deeper: Using LLM-As-A-Judge with Custom Criteria\n",
    "\n",
    "Same as above, but with our own custom criteria now!\n",
    "\n",
    "* `criteria` for binary scoring\n",
    "* `score_string` for numeric scoring\n",
    "* `labeled_criteria` evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference label\n",
    "* `labeled_score_string` evaluator has the LLM score the prediction on a numeric scale based on how well it satisfies the criteria compared to the reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71991cb-4d51-41f1-b84f-69d811793855",
   "metadata": {},
   "source": [
    "### Unlabeled (no ground truth- LLM assesses at face value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d537165b-3d6e-4791-a8a2-f79fc4f070e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "eval_objectivity_scorestring = LangChainStringEvaluator(\n",
    "    \"score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"objectivity\": \"On a scale of 1-10, how unbiased and impartial is this submission? Rate it based on the extent to which it presents information and facts without personal opinions or slanted perspectives, aligning with the objective truth of the topic.\",\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "    }  \n",
    ")\n",
    "\n",
    "eval_objectivity_criteria = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"objectivity\": \"Is this submission unbiased and impartial, presenting information and facts without personal opinions or slanted perspectives, ensuring it aligns with the objective truth of the topic?\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03662ffa-1b26-41c8-bbcb-985f4598e188",
   "metadata": {},
   "source": [
    "### Labeled (with reference to data)\n",
    "\n",
    "This requires an extra prepare_data argument, that will take in a `prediction`, `reference`, and `input`. Similar to above, these will be referenced from the `Run` (LLM output) or the `Example` (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b3e5f3-6d54-4c56-a9dd-06545a7161cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labeled_objectivity_criteria = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"objectivity\": (\n",
    "                \"Is this submission unbiased and impartial, presenting information and facts without personal opinions or slanted perspectives, ensuring it aligns with the objective truth of the topic?\"\n",
    "            )\n",
    "        }\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],   \n",
    "    }\n",
    ")\n",
    "\n",
    "eval_labeled_objectivity_scorestring = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\", \n",
    "    config={\n",
    "        \"criteria\": { \n",
    "            \"objectivity\": \"On a scale of 1-10, how unbiased and impartial is this submission? Rate it based on the extent to which it presents information and facts without personal opinions or slanted perspectives, aligning with the objective truth of the topic.\"\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"], \n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ec288-e22c-48f1-98d7-aa836fc7031d",
   "metadata": {},
   "source": [
    "### Running multiple evaluators at once in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0ba522-6a83-4d66-ba66-9f7fdb891f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_evaluators = [eval_objectivity_scorestring, eval_objectivity_criteria]\n",
    "labeled_evaluators = [eval_labeled_objectivity_criteria, eval_labeled_objectivity_scorestring]\n",
    "dataset_name = \"agent_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d20d2-9682-40e1-b389-392837a5d678",
   "metadata": {},
   "source": [
    "**Unlabeled evaluators with GPT-4o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690e73a5-4b69-4d2b-bdac-76ce0594e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-objectivity-unlabeled-oai-e5900284' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=d7fc5637-a4c0-447e-9f2b-9dcdfa8fa57f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ef032702d84587b5c8b3a039b640eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oai_unlabeled_results = evaluate(\n",
    "    qa_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=unlabeled_evaluators,\n",
    "    experiment_prefix=\"test-agent-objectivity-unlabeled-oai\",\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with gpt-4o, unlabeled\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e1f39-e8a3-485d-80b8-b25ed7b69280",
   "metadata": {},
   "source": [
    "**Unlabeled evaluators with Mistral 7b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "283d50f2-5706-453e-ada1-46895a0e7d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-objectivity-unlabeled-mistral-e18e0d1b' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=afce084b-30c0-42ab-8c26-6127035dcf88\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677f5c5de8564c22854252bf50400635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The primary function of Large Language Models (LLMs) in autonomous agents is to process natural language inputs and generate appropriate outputs, interacting with external components such as memory and tools. They help in understanding instructions, generating responses, and executing tasks by parsing and interpreting textual data. LLM-powered autonomous agents face several challenges in real-world applications, including finite context length which limits historical information and detailed instructions, reliability of natural language interfaces due to formatting errors and rebellious behavior, and difficulties with long-term planning and task decomposition. These limitations impact the robustness and effectiveness of these agents. (References: [1], [2], [3], [4], [5], [6], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21]) In LLM-powered autonomous agents, planning plays a crucial role by allowing the agent to adjust its actions based on long-term goals and unexpected errors. It helps the agent to explore the solution space effectively and make robust decisions. However, current LLMs face challenges in reliably handling natural language interfaces, finite context length, and long-term planning, which limit their overall effectiveness as autonomous agents. Ongoing research aims to address these limitations by integrating techniques such as self-reflection, tool augmentation, and reinforcement learning. Autonomous agents use tool APIs by integrating them with large language models, enabling the agents to perform specific tasks and access external knowledge sources. This synergistic approach allows agents to expand their capabilities beyond their inherent limitations and solve complex problems in various domains. LLM-powered agents typically utilize two types of memory: external memory, which can be a database or knowledge base, and internal memory, which is the agent's own state and context. External memory stores facts and information that the agent does not have access to internally, while internal memory helps the agent keep track of its current state and the context of the conversation."
     ]
    }
   ],
   "source": [
    "mistral_unlabeled_results = evaluate(\n",
    "    qa_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=unlabeled_evaluators,\n",
    "    experiment_prefix=\"test-agent-objectivity-unlabeled-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with Mistral 7B, unlabeled\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832fb63-b3ee-4d18-b909-d5b214ba046d",
   "metadata": {},
   "source": [
    "**Labeled Evaluators with gpt-4o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d03ff3-56a6-474b-b5e9-ed2cac75fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-objectivity-labeled-oai-a19e3d7e' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=a337ef96-4f61-4a1f-bef8-37e3e009ce74\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7915defdd9ef44bcaf832258e37999e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These are now in comparison to the \"reference output\"\n",
    "oai_labeled_results = evaluate(\n",
    "    qa_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=labeled_evaluators,\n",
    "    experiment_prefix=\"test-agent-objectivity-labeled-oai\",\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with gpt-3.5-turbo, labeled\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21588304-31a5-4f44-b1d6-666d56c8070a",
   "metadata": {},
   "source": [
    "**Labeled evaluators with Mistral 7b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ce4d95d-54f9-4f44-940c-4bfd4106c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-objectivity-labeled-mistral-017ce55d' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=652ea396-158f-4ade-aff2-81fa78b8d11d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8953d187e547b4a5bf4ab5fcf27b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Some challenges faced by LLM-powered autonomous agents in real-world applications include finite context length, which limits historical information and detailed instructions, making it difficult for long-term planning and effective task decomposition. Another challenge is the reliability of natural language interfaces, as LLMs may exhibit formatting errors or rebellious behavior. These issues make it important to continually research and develop methods to improve the performance and robustness of these agents. LLM-powered agents utilize different types of memory, including dynamic memory for self-reflection and static memory stored in vector stores or databases. They also rely on natural language interfaces to communicate with external components, which can be unreliable due to formatting errors or rebellious behavior. The primary function of a Large Language Model (LLM) in autonomous agents is to process natural language instructions and generate responses or actions based on that input. It serves as the conversational and cognitive component, interpreting data from external components and tools, and executing tasks assigned by the agent's architecture. LLMs help agents understand context, perform reasoning, and interact with their environment, making them essential building blocks for developing sophisticated autonomous systems. In LLM-powered autonomous agents, planning plays a crucial role in guiding the agent's actions based on its current context and long-term goals. It helps the agent adjust its plans when faced with unexpected errors and enables effective exploration of the solution space. However, challenges such as finite context length and reliability of natural language interfaces can limit the agent's planning capabilities. Techniques like self-reflection, in-context reinforcement learning, algorithm distillation, and tool augmentation are being explored to improve planning in LLM-powered autonomous agents. Autonomous agents can use Tool APIs to interact with external tools and systems, expanding their capabilities beyond language processing. This enables agents to perform complex tasks that require specialized knowledge or access to external data, making them more versatile and effective in various domains."
     ]
    }
   ],
   "source": [
    "mistral_labeled_results = evaluate(\n",
    "    qa_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=labeled_evaluators,\n",
    "    experiment_prefix=\"test-agent-objectivity-labeled-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with Mistral 7B, labeled\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45619028-4521-417b-b7a0-b47a60618a8d",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating existing Evaluations\n",
    "\n",
    "What if your evaluation of interest is not at the individual run level, but on the overall experiment level?\n",
    "\n",
    "https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_existing_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62231e17-bda6-4571-9943-a263606ac3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n",
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n",
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-qa-mistral-multicriteria-63912a34' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=b7f96e55-d16a-4dac-b9ad-44b0cacff931\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726e7075c1c2454eb88371d3daaae0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM-powered agents typically utilize two types of memory: dynamic memory for storing and retrieving information during conversation or task execution, and external knowledge sources such as databases or APIs to access a larger pool of information. Some agents also use self-reflection and learning mechanisms to store past experiences for future reference. In LLM-powered autonomous agents, 'Planning' is a crucial component that enables the agent to generate a sequence of actions based on its current context and goals. It allows the agent to adjust its plans when faced with unexpected errors, improving its robustness compared to humans who learn from trial and error. Effective planning in LLMs is challenging due to their finite context length and reliability issues with natural language interfaces. Various approaches like reinforcement learning, algorithm distillation, and modular architecture have been explored to enhance the planning capabilities of LLMs. The primary function of Large Language Models (LLMs) in autonomous agents is to process natural language inputs, generate appropriate responses, and perform tasks by interacting with external components such as memory and tools. They are designed to learn from experience and adapt to new situations, making them valuable for solving complex problems and executing multi-step instructions. LLM-powered autonomous agents face several challenges in real-world applications, including finite context length which limits historical information and detailed instructions, reliability of natural language interfaces due to formatting errors and occasional rebellious behavior, and difficulties in long-term planning and task decomposition. (References: [1], [2], [3], [4], [5], [6], [8], [9], [10], [11], [12], [13], [15], [17], [18], [19]) Autonomous agents can use tool APIs by integrating them with large language models, enabling the agents to access and utilize external tools for specific tasks. This integration enhances the agent's capabilities, making it more effective in solving complex problems and performing various tasks. Examples of tool-augmented agents include ChemCrow for chemistry tasks, HuggingGPT for AI tasks, and GPT-Engineer for software engineering tasks."
     ]
    }
   ],
   "source": [
    "helpfulness_scorestring = LangChainStringEvaluator(\"score_string\", config={ \"criteria\": \"helpfulness\" })\n",
    "conciseness_scorestring = LangChainStringEvaluator(\"score_string\", config={ \"criteria\": \"conciseness\" })\n",
    "coherence_scorestring = LangChainStringEvaluator(\"score_string\", config={ \"criteria\": \"coherence\" })\n",
    "\n",
    "evaluators = [helpfulness_scorestring, conciseness_scorestring, coherence_scorestring]\n",
    "\n",
    "mistral_multicriteria_eval = evaluate(\n",
    "    qa_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"test-agent-qa-mistral-multicriteria\",\n",
    "    metadata={\n",
    "        \"variant\": \"full website in context window with Mistral 7b, helpfulness, conciseness, and coherence check\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37480cca-b069-4734-ac0f-ea3ab72da3c8",
   "metadata": {},
   "source": [
    "### Set up a Summary Evaluator to look over the entire dataset and determine whether an output was generated\n",
    "\n",
    "Our criteria for a pass is that the model output an answer successfully 80% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "493a1911-3cad-4002-be94-ef557fc81aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agent-qa-mistral-multicriteria-63912a34' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=b7f96e55-d16a-4dac-b9ad-44b0cacff931\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf7f9c720b8410a849cc15d5b6e116b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults test-agent-qa-mistral-multicriteria-63912a34>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate_existing\n",
    "\n",
    "experiment_name = mistral_multicriteria_eval.experiment_name\n",
    "\n",
    "def passed_eval(runs: list, examples: list):\n",
    "    output = 0\n",
    "    for i, run in enumerate(runs):\n",
    "        if run.outputs[\"answer\"]:\n",
    "            output +=1\n",
    "    if output / len(runs) > 0.8:\n",
    "        return {\"key\": \"pass\", \"score\": True}\n",
    "    else:\n",
    "        return {\"key\": \"fail\", \"score\": False}\n",
    "\n",
    "evaluate_existing(experiment_name, summary_evaluators=[passed_eval])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba1750-f9ea-4dd5-9160-1def16ca5d4a",
   "metadata": {},
   "source": [
    "---\n",
    "# Pairwise Evaluations\n",
    "\n",
    "https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise\n",
    "\n",
    "Allows you to evaluate exisiting experiments against eachother. Example: LLM-As-Judge evaluating it's preference between two outputs from LLMs from an existing evaluation. This could be useful to compare two small model outputs using a large model. \n",
    "\n",
    "Using this prompt: https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2?organizationId=ef6f5694-a2fa-5316-9158-12297cd17350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfe453b6-7ff4-4054-991a-6720fc3e554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate_comparative\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Run, Example\n",
    "prompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")\n",
    "\n",
    "# Example from documentation, using GPT-4o to evaluate preference between two model's outputs\n",
    "def evaluate_pairwise(runs: list[Run], example: Example):\n",
    "    scores = {}\n",
    "\n",
    "    # Create the model to run your evaluator\n",
    "    model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "    runnable = prompt | model\n",
    "    response = runnable.invoke({\n",
    "        \"question\": example.inputs[\"question\"],\n",
    "        \"answer_a\": runs[0].outputs[\"answer\"] if runs[0].outputs is not None else \"N/A\",\n",
    "        \"answer_b\": runs[1].outputs[\"answer\"] if runs[1].outputs is not None else \"N/A\",\n",
    "    })\n",
    "    score = response[\"Preference\"]\n",
    "    if score == 1:\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == 2:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3b01c-bc3e-4668-a09a-2f07bb1a6392",
   "metadata": {},
   "source": [
    "### Running Comparative Evaluation\n",
    "\n",
    "Going to use our prior experiments on helpfulness using Mistral 7b and GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af7abd47-fc42-4744-b111-35a5a31c7011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/8d445ea4-b7d1-4d36-a641-437e4efa4a5b/compare?selectedSessions=1a6be94f-4f7b-4ef9-863c-ed2be1d3b441%2C7245e36d-b143-4cfa-a15c-9f5714885636&comparativeExperiment=dca56cef-79d4-4252-9171-c60b3217ff75\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50344bb6f834cf09475ec5a7df7c1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x1772c7fe0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_comparative(\n",
    "    # Replace the following array with the names or IDs of your experiments\n",
    "    [oai_helpfulness_eval.experiment_name, mistral_helpfulness_eval.experiment_name],\n",
    "    evaluators=[evaluate_pairwise],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef046fc0-8c0e-46d8-ac87-1a92fbda8a57",
   "metadata": {},
   "source": [
    "---\n",
    "# Unit Tests w/pytest - VSCode Example\n",
    "\n",
    "https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing\n",
    "\n",
    "You can use a built in decorator `@unit` to attach to `pytest` tests. These will then be logged with langsmith and can be viewed/compared similar to the existing evaluations we've been over.\n",
    "\n",
    "Built in with the `@unit` Decorator:\n",
    "\n",
    "| Feedback           | Description                                                   | Example                                                                                                                      |\n",
    "|--------------------|---------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
    "| pass               | Binary pass/fail score, 1 for pass, 0 for fail                | `assert False # Fails`                                                                                                       |\n",
    "| expectation        | Binary expectation score, 1 if expectation is met, 0 if not   | `expect(prediction).against(lambda x: re.search(r\"\\b[a-f\\d]{8}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{12}\\b\", x))`          |\n",
    "| embedding_distance | Cosine distance between two embeddings                        | `expect.embedding_distance(prediction=prediction, expectation=expectation)`                                                  |\n",
    "| edit_distance      | Edit distance between two strings                             | `expect.edit_distance(prediction=prediction, expectation=expectation)`                                                       |\n",
    "\n",
    "\n",
    "\n",
    "### `expect` methods\n",
    "\n",
    "\n",
    "| Method              | Description                                                                                   | Parameters                                           |\n",
    "|---------------------|-----------------------------------------------------------------------------------------------|------------------------------------------------------|\n",
    "| `to_be_less_than`   | Assert that the expectation value is less than the given value.                               | `value`                                              |\n",
    "| `to_be_greater_than`| Assert that the expectation value is greater than the given value.                            | `value`                                              |\n",
    "| `to_be_between`     | Assert that the expectation value is between the given min and max values.                    | `min_value, max_value`                               |\n",
    "| `to_be_approximately`| Assert that the expectation value is approximately equal to the given value.                  | `value, precision=2`                                 |\n",
    "| `to_equal`          | Assert that the expectation value equals the given value.                                     | `value`                                              |\n",
    "| `to_contain`        | Assert that the expectation value contains the given value.                                   | `value`                                              |\n",
    "| `against`           | Assert the expectation value against a custom function.                                       | `func`                                               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801d92-050a-44bb-9da9-468132ca3326",
   "metadata": {},
   "source": [
    "---\n",
    "# Hopping over to Llama3 Research Agent Notebook to Assess Attaching Evaluations within Existing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71068d61-ee98-4e56-9041-40574d057d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b053f2-0c96-4929-9ca9-98d78c496276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
