{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c707a92-6f82-44d8-8de0-fa612064df5e",
   "metadata": {},
   "source": [
    "# Local Web Research Agent w/ Llama 3 8b\n",
    "\n",
    "### [Llama 3 Release](https://llama.meta.com/llama3/)\n",
    "\n",
    "### [Ollama Llama 3 Model](https://ollama.com/library/llama3)\n",
    "---\n",
    "\n",
    "![diagram](local_agent_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715059b3-857c-456d-a740-24e2551d739d",
   "metadata": {},
   "source": [
    "---\n",
    "[Llama 3 Prompt Format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "\n",
    "### Special Tokens used with Meta Llama 3\n",
    "* **<|begin_of_text|>**: This is equivalent to the BOS token\n",
    "* **<|eot_id|>**: This signifies the end of the message in a turn.\n",
    "* **<|start_header_id|>{role}<|end_header_id|>**: These tokens enclose the role for a particular message. The possible roles can be: system, user, assistant.\n",
    "* **<|end_of_text|>**: This is equivalent to the EOS token. On generating this token, Llama 3 will cease to generate more tokens.\n",
    "A prompt should contain a single system message, can contain multiple alternating user and assistant messages, and always ends with the last user message followed by the assistant header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f2cb84-6abf-4a6c-8d1f-cdc6474b77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying final output format\n",
    "from IPython.display import display, Markdown, Latex\n",
    "# LangChain Dependencies\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langgraph.graph import END, StateGraph\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d39b8539-1bfe-4001-b7b2-6752a77846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"L3 Research Agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b341d1d-0a59-4c03-8558-759ea00171bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LLM\n",
    "local_llm = 'llama3'\n",
    "llama3 = ChatOllama(model=local_llm, temperature=0)\n",
    "llama3_json = ChatOllama(model=local_llm, format='json', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c7813ac-791f-4035-a5ec-04810d5de5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Tool\n",
    "# pip install -U duckduckgo_search==5.3.0b4\n",
    "# ^ if running into 202 rate limit error\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=15)\n",
    "web_search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "\n",
    "# Test Run\n",
    "# resp = web_search_tool.invoke(\"home depot news\")\n",
    "# resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d798a81-6ed6-4a4f-a1d9-93b4e3059fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Prompt\n",
    "\n",
    "generate_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an AI assistant for Research Question Tasks, that synthesizes web search results. \n",
    "    Strictly use the following pieces of web search context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    keep the answer concise, but provide all of the details you can in the form of a research report. \n",
    "    Only make direct references to material if provided in the context.\n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    \n",
    "    Question: {question} \n",
    "    Web Search Context: {context} \n",
    "    Answer: \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "generate_chain = generate_prompt | llama3 | StrOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"How are you?\"\n",
    "# context = \"\"\n",
    "# generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49fa1965-6bd4-4dfc-9eb8-96c6cff7b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "    You are an expert at routing a user question to either the generation stage or web search. \n",
    "    Use the web search for questions that require more context for a better answer, or recent events.\n",
    "    Otherwise, you can skip and go straight to the generation phase to respond.\n",
    "    You do not need to be stringent with the keywords in the question related to these topics.\n",
    "    Give a binary choice 'web_search' or 'generate' based on the question. \n",
    "    Return the JSON with a single key 'choice' with no premable or explanation. \n",
    "    \n",
    "    Question to route: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "question_router = router_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"What's up?\"\n",
    "# print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33ab4128-e0b0-4f49-9f36-1d3bf5636715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Transformation\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an expert at crafting web search queries for research questions.\n",
    "    More often than not, a user will ask a basic question that they wish to learn more about, however it might not be in the best format. \n",
    "    Reword their query to be the most effective web search string possible.\n",
    "    Return the JSON with a single key 'query' with no premable or explanation. \n",
    "    \n",
    "    Question to transform: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "query_chain = query_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"What's happened recently with Macom?\"\n",
    "# print(query_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1c8e922-3f00-48d6-83cb-cc78a2292838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Node - Query Transformation\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform user question to web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended search query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Optimizing Query for Web Search\")\n",
    "    question = state['question']\n",
    "    gen_query = query_chain.invoke({\"question\": question})\n",
    "    search_query = gen_query[\"query\"]\n",
    "    return {\"search_query\": search_query}\n",
    "\n",
    "\n",
    "# Node - Web Search\"\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to context\n",
    "    \"\"\"\n",
    "\n",
    "    search_query = state['search_query']\n",
    "    print(f'Step: Searching the Web for: \"{search_query}\"')\n",
    "    \n",
    "    # Web search tool call\n",
    "    search_result = web_search_tool.invoke(search_query)\n",
    "    return {\"context\": search_result}\n",
    "\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    route question to web search or generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step: Routing Query\")\n",
    "    question = state['question']\n",
    "    output = question_router.invoke({\"question\": question})\n",
    "    if output['choice'] == \"web_search\":\n",
    "        print(\"Step: Routing Query to Web Search\")\n",
    "        return \"websearch\"\n",
    "    elif output['choice'] == 'generate':\n",
    "        print(\"Step: Routing Query to Generation\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f665713-e80b-4d86-8015-77ba55506004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Build the edges\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"websearch\")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "local_agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f53aa05-20b2-420e-9a8f-bf12b1e547ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable \n",
    "def run_agent(query):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a1b135d-131e-4276-b40c-12ea8b78c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Searching the Web for: \"Apple Q3 earnings report\"\n",
      "Step: Generating Final Response\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided web search context, Apple's Q3 earnings are as follows:\n",
       "\n",
       "* Quarterly revenue: $81.8 billion (down 1% year over year)\n",
       "* Quarterly earnings per diluted share: $1.26 (up 5% year over year)\n",
       "\n",
       "Note that these figures were announced by Apple on August 3, 2023, and the company's CEO Tim Cook and CFO Luca Maestri shared additional details during their Q3 2023 financial results call."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it out!\n",
    "run_agent(\"What's are Apple's q3 earnings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fca5a-b73e-4c19-9f47-ebe0ca08bc79",
   "metadata": {},
   "source": [
    "---\n",
    "# Attaching Evals to Existing Runs\n",
    "\n",
    "What if you have an existing application that's being traced, and you want to insert evaluations at specific parts of the operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d1aa9-67df-4dee-8934-81ec77106dd7",
   "metadata": {},
   "source": [
    "### Creating a quick QA dataset to test against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29e9bab4-5734-4592-a69b-36ee756b56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "    (\"What Apple's Q3 Earnings?\", \"Apple today announced financial results for its fiscal 2023 third quarter ended July 1, 2023. The Company posted quarterly revenue of $81.8 billion, down 1 percent year over year, and quarterly earnings per diluted share of $1.26, up 5 percent year over year.\"),\n",
    "    (\"What are new apple products?\", \"Apple is refreshing both iPad Pro models with OLED screens, bringing a major update in display quality. There will be two models with screen sizes around 11 and 13 inches, and we are expecting design updates. With the switch to OLED, Apple is cutting down on thickness, and the new iPad Pro models will be much thinner. We're also expecting them to adopt the M3 chip for faster performance, and Apple is planning to debut a new Magic Keyboard that gives the iPad Pro a more Mac-like feel and a new Apple Pencil.  With the 2024 iPad Air refresh, we're getting two models for the first time. The smaller iPad Air will have a 10.9-inch display like the current iPad Air, while the larger version will have a 12.9-inch display like the current 12.9-inch iPad Pro. The iPad Air models will be more affordable than the iPad Pro models, and won't have \\\"Pro\\\" features like ProMotion refresh rates and OLED displays. Rumors are mixed on whether the iPad Air will get the M2 or the M3 chip, but either option will be an improvement over the M1 in the current model.\"),\n",
    "]\n",
    "\n",
    "dataset_name = \"Apple - L3 Agent Testing\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0e334-b5ad-4b3c-9a0e-95c047dd7ef9",
   "metadata": {},
   "source": [
    "### Defining Some Custom Evaluators\n",
    "\n",
    "Few notes here, using structured function calling alongside OpenAI to create a quick LLM-as-judge Evaluator\n",
    "\n",
    "Also, need to make sure that digging into your runs/child_runs is accurate. Using LangSmith expand all runs to see how this flows exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a90ba29-7799-4c47-b196-0494bbc3a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Search Tool Test\n",
    "def search_retrieval(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks if the retrieved web search contains answer for the question\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    agent_run = next(run for run in root_run.child_runs if run.name == \"run_agent\")\n",
    "    LangGraph = next(run for run in agent_run.child_runs if run.name == \"LangGraph\")\n",
    "    search_run = next(run for run in LangGraph.child_runs if run.name == \"websearch\")\n",
    "    context = search_run.outputs[\"context\"]\n",
    "    question = agent_run.inputs[\"query\"]\n",
    "\n",
    "    # Data model\n",
    "    class GradeWebsearch(BaseModel):\n",
    "        \"\"\"Binary score for whether websearch contains question context.\"\"\"\n",
    "\n",
    "        binary_score: int = Field(description=\"Context contains answer to question, 1 or 0\")\n",
    "\n",
    "    # LLM with function call\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    structured_websearch_grader = llm.with_structured_output(GradeWebsearch)\n",
    "\n",
    "    # Prompt\n",
    "    system = \"\"\"You are a grader assessing whether an Web search contains the context needed to answer a user query. \\n\n",
    "        Give a binary score 1 or 0, where 1 means that the answer is in the web search results.\"\"\"\n",
    "    websearch_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Web search: \\n\\n {context} \\n\\n Question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    websearch_grader = websearch_prompt | structured_websearch_grader\n",
    "    score = websearch_grader.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"key\": \"websearch_verification\", \"score\": int(score.binary_score)}\n",
    "\n",
    "# Hallucination Test\n",
    "def hallucination(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the context\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    agent_run = next(run for run in root_run.child_runs if run.name == \"run_agent\")\n",
    "    LangGraph = next(run for run in agent_run.child_runs if run.name == \"LangGraph\")\n",
    "    search_run = next(run for run in LangGraph.child_runs if run.name == \"websearch\")\n",
    "    context = search_run.outputs[\"context\"]\n",
    "    generation = LangGraph.outputs[\"generation\"]\n",
    "\n",
    "    # Data model\n",
    "    class GradeHallucinations(BaseModel):\n",
    "        \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "        binary_score: int = Field(description=\"Answer is grounded in the facts, 1 or 0\")\n",
    "\n",
    "    # LLM with function call\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "    # Prompt\n",
    "    system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "        Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "    hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Set of facts: \\n\\n {context} \\n\\n LLM generation: {generation}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "    score = hallucination_grader.invoke({\"context\": context, \"generation\": generation})\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": int(score.binary_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4202b-18fa-46ff-a814-d9b2fc1e9e42",
   "metadata": {},
   "source": [
    "### Running the Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "482e415d-e08c-4acf-83a3-15e2fb5c28df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'websearch-test-1-4ebcc119' at:\n",
      "https://smith.langchain.com/o/ef6f5694-a2fa-5316-9158-12297cd17350/datasets/e301d2c7-3cfd-4a70-8ecf-2ea308bf9ad4/compare?selectedSessions=86b68fc3-c086-4a77-a730-7bb46c77028f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6a92a3483b4201a344e98023a2fdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query\n",
      "Step: Routing Query to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Routing Query to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Searching the Web for: \"new Apple products\"\n",
      "Step: Searching the Web for: \"Apple Q3 earnings report\"\n",
      "Step: Generating Final Response\n",
      "Step: Generating Final Response\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided web search context, new Apple products include:\n",
       "\n",
       "* iPad Air: Available in new blue and purple finishes, along with starlight and space gray, starting at $599 for the 11-inch model and $799 for the 13-inch model.\n",
       "* iPhone 15 and iPhone 15 Plus: Feature a gorgeous new design, Dynamic Island, 48MP Main camera, and A16 Bionic chip. They will be available in five colors and have a USB-C connector, contoured edge, and durable color-infused back glass. Pre-orders begin on September 15, with availability starting on September 22.\n",
       "* iPhone 15 Pro and iPhone 15 Pro Max: Available in four stunning new finishes, including black titanium, white titanium, blue titanium, and natural titanium. Pre-orders begin on September 15, with availability starting on September 22.\n",
       "* Apple Watch Series 9: Available in 41mm and 45mm sizes in starlight, midnight, silver, (PRODUCT)RED, and a new pink aluminum case, as well as stainless steel in gold, silver, and graphite cases.\n",
       "* Apple Pencil: A new, more affordable option with pixel-perfect accuracy, low latency, and tilt sensitivity for note taking, sketching, and more. It works with all iPad models that have a USB-C port, including iPad Pro, iPad Air, and iPad mini, and is available for purchase beginning in early November.\n",
       "* Mac Studio: Receiving an update, including the silicon, replacing the M1 Max and M1 Ultra with the M2 Max and M2 Ultra.\n",
       "\n",
       "Note: The article also mentions Apple's upcoming mixed reality headset, which can play back stereoscopic 3D video shot on iPhone 15 Pro."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided web search context, Apple's Q3 earnings are as follows:\n",
       "\n",
       "* Quarterly revenue: $81.8 billion, down 1% year over year\n",
       "* Quarterly earnings per diluted share: $1.26, up 5% year over year\n",
       "\n",
       "These figures were announced by Apple in its fiscal 2023 third-quarter earnings report, which was released on August 3, 2023."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    lambda inputs: run_agent(inputs[\"input\"]),\n",
    "    data=\"Apple - L3 Agent Testing\",\n",
    "    evaluators=[search_retrieval, hallucination],\n",
    "    experiment_prefix=\"websearch-test-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72781a0b-3082-4503-bd76-ddeb2a52efb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
